import pandas as pd
from tqdm import tqdm

from coordinate_utils import CoordinateProcessor
from features import Feature, _inst, feature_classes


def preprocess_cyclone_data(df: pd.DataFrame, show_progress: bool = True) -> pd.DataFrame:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ü–∏–∫–ª–æ–Ω–æ–≤, –¥–æ–±–∞–≤–ª—è—è –ø—Ä–∏–∑–Ω–∞–∫–∏.

    Parameters
    ----------
    df : pd.DataFrame
        –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ü–∏–∫–ª–æ–Ω–æ–≤
    show_progress : bool
        –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä

    Returns
    -------
    pd.DataFrame
        –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    """
    df_sorted = df.sort_values(["intl_id", "analysis_time"]).reset_index(drop=True)
    Feature.reset_cyclone_cache()

    feats = [_inst(cls) for cls in feature_classes]
    columns: dict[str, list] = {cls.name(): [] for cls in feature_classes}

    iterator = df_sorted.itertuples(index=False, name=None)
    if show_progress:
        iterator = tqdm(iterator, total=len(df_sorted), desc="Calculating features")

    for row in iterator:
        row_s = pd.Series(row, index=df_sorted.columns)
        for f_obj, cls in zip(feats, feature_classes):
            columns[cls.name()].append(f_obj.calculate(row_s))

    out = df_sorted.copy()
    for col, values in columns.items():
        out[col] = values
    return out


def build_dataset(
    df: pd.DataFrame, horizons_hours: list[int] = [6], validate_data: bool = True
) -> tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:
    """
    –°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π.

    Parameters
    ----------
    df : pd.DataFrame
        –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ü–∏–∫–ª–æ–Ω–æ–≤
    horizons_hours : list[int]
        –°–ø–∏—Å–æ–∫ –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–≤ –ø—Ä–æ–≥–Ω–æ–∑–∞ –≤ —á–∞—Å–∞—Ö
    validate_data : bool
        –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å

    Returns
    -------
    Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]
        X, y, times, intl_ids, storm_names
    """
    df = df.copy()
    df["analysis_time"] = pd.to_datetime(df["analysis_time"])

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ–ª–≥–æ—Ç—É –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [-180, 180]
    df["lon_deg"] = CoordinateProcessor.normalize_longitude(df["lon_deg"])

    if validate_data:
        _validate_input_data(df)

    datasets = []

    for horizon in horizons_hours:
        df_h = df.copy()
        df_h["target_time"] = df_h["analysis_time"] + pd.Timedelta(hours=horizon)
        df_h["target_time_hours"] = horizon

        merged = pd.merge(
            df_h,
            df[["intl_id", "analysis_time", "lat_deg", "lon_deg"]],
            left_on=["intl_id", "target_time"],
            right_on=["intl_id", "analysis_time"],
            how="inner",
            suffixes=("", "_target"),
        )

        # –¢–æ–ª—å–∫–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è
        merged["dlat_target"] = merged["lat_deg_target"] - merged["lat_deg"]
        merged["dlon_target"] = CoordinateProcessor.circ_diff(merged["lon_deg"], merged["lon_deg_target"])

        datasets.append(merged)

    df_final = pd.concat(datasets, ignore_index=True)

    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    if validate_data:
        _validate_final_dataset(df_final)

    feature_cols = ["target_time_hours"] + [f.name() for f in feature_classes]
    target_cols = ["dlat_target", "dlon_target"]  # –¢–æ–ª—å–∫–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è

    X = df_final[feature_cols]
    y = df_final[target_cols]
    times = df_final["analysis_time"]
    intl_ids = df_final["intl_id"]
    storm_names = df_final["storm_name"] if "storm_name" in df_final.columns else pd.Series([""] * len(df_final))

    return X, y, times, intl_ids, storm_names


def split_data_by_years(
    X: pd.DataFrame,
    y: pd.DataFrame,
    times: pd.Series,
    train_max_year: int = 2017,
    val_max_year: int = 2019,
) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    –†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –≥–æ–¥–∞–º –Ω–∞ train/val/test.

    Parameters
    ----------
    X, y, times, intl_ids, storm_names : –¥–∞–Ω–Ω—ã–µ
    train_max_year : int
        –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
    val_max_year : int
        –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏

    Returns
    -------
    Tuple
        X_train, y_train, X_val, y_val, X_test, y_test
    """
    mask_train = times.dt.year <= train_max_year
    mask_val = (times.dt.year > train_max_year) & (times.dt.year <= val_max_year)
    mask_test = times.dt.year > val_max_year

    X_train, y_train = X[mask_train], y[mask_train]
    X_val, y_val = X[mask_val], y[mask_val]
    X_test, y_test = X[mask_test], y[mask_test]

    return X_train, y_train, X_val, y_val, X_test, y_test


def print_dataset_info(
    X: pd.DataFrame,
    y: pd.DataFrame,
    times: pd.Series,
    X_train: pd.DataFrame,
    X_val: pd.DataFrame,
    X_test: pd.DataFrame,
    train_mask: pd.Series,
    val_mask: pd.Series,
    test_mask: pd.Series,
) -> None:
    """
    –í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ.
    """
    print("\nüìä –ò–¢–û–ì–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ü–ê–†–ê–ú–ï–¢–†–ò–ó–û–í–ê–ù–ù–û–ú –î–ê–¢–ê–°–ï–¢–ï:")
    print(f"  ‚Ä¢ –ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(X):,}")
    print(f"  ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ ({X.shape[1]}): {list(X.columns)}")
    print(f"  ‚Ä¢ –¶–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö ({y.shape[1]}): {list(y.columns)}")

    print("\nüìà –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•:")
    print(f"  ‚Ä¢ Train —Ä–∞–∑–º–µ—Ä: {len(X_train):,}")
    print(f"  ‚Ä¢ Val   —Ä–∞–∑–º–µ—Ä: {len(X_val):,}")
    print(f"  ‚Ä¢ Test  —Ä–∞–∑–º–µ—Ä: {len(X_test):,}")
    print(f"  ‚Ä¢ –ì–æ–¥—ã Train: {times[train_mask].dt.year.min()} ‚Äî {times[train_mask].dt.year.max()}")
    print(f"  ‚Ä¢ –ì–æ–¥—ã Val  : {times[val_mask].dt.year.min()} ‚Äî {times[val_mask].dt.year.max()}")
    print(f"  ‚Ä¢ –ì–æ–¥—ã Test : {times[test_mask].dt.year.min()} ‚Äî {times[test_mask].dt.year.max()}")

    print("\nüéØ –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ì–û–†–ò–ó–û–ù–¢–ê–ú –ü–†–û–ì–ù–û–ó–ê:")
    if "target_time_hours" in X.columns:
        horizon_distribution = X["target_time_hours"].value_counts().sort_index()
        for horizon, count in horizon_distribution.items():
            percentage = count / len(X) * 100
            print(f"  ‚Ä¢ {horizon:2.0f}—á: {count:6,} –ø—Ä–∏–º–µ—Ä–æ–≤ ({percentage:5.1f}%)")


def _validate_input_data(df: pd.DataFrame) -> None:
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ."""
    required_cols = ["intl_id", "analysis_time", "lat_deg", "lon_deg"]
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")

    if df["lat_deg"].isna().any():
        raise ValueError("–ù–∞–π–¥–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ lat_deg")

    if df["lon_deg"].isna().any():
        raise ValueError("–ù–∞–π–¥–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ lon_deg")

    if (df["lat_deg"].abs() > 90).any():
        raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —à–∏—Ä–æ—Ç—ã (–≤–Ω–µ [-90, 90])")

    if (df["lon_deg"].abs() > 180).any():
        raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ–ª–≥–æ—Ç—ã (–≤–Ω–µ [-180, 180])")


def _validate_final_dataset(df: pd.DataFrame) -> None:
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç."""
    required_target_cols = ["dlat_target", "dlon_target"]
    missing_cols = [col for col in required_target_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å–º–µ—â–µ–Ω–∏–π
    dlat_extreme = df["dlat_target"].abs() > 10  # –±–æ–ª–µ–µ 10 –≥—Ä–∞–¥—É—Å–æ–≤ –∑–∞ –æ–¥–∏–Ω —à–∞–≥
    dlon_extreme = df["dlon_target"].abs() > 20  # –±–æ–ª–µ–µ 20 –≥—Ä–∞–¥—É—Å–æ–≤ –∑–∞ –æ–¥–∏–Ω —à–∞–≥

    if dlat_extreme.any():
        n_extreme = dlat_extreme.sum()
        print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–∞–π–¥–µ–Ω–æ {n_extreme} —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö —Å–º–µ—â–µ–Ω–∏–π –ø–æ —à–∏—Ä–æ—Ç–µ (>10¬∞)")

    if dlon_extreme.any():
        n_extreme = dlon_extreme.sum()
        print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–∞–π–¥–µ–Ω–æ {n_extreme} —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö —Å–º–µ—â–µ–Ω–∏–π –ø–æ –¥–æ–ª–≥–æ—Ç–µ (>20¬∞)")


class DataProcessor:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Ü–∏–∫–ª–æ–Ω–æ–≤.
    """

    def __init__(
        self,
        horizons_hours: list[int] = [6, 12, 18, 24, 48],
        train_max_year: int = 2017,
        val_max_year: int = 2019,
        validate_data: bool = True,
    ):
        """
        Parameters
        ----------
        horizons_hours : list[int]
            –ì–æ—Ä–∏–∑–æ–Ω—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∞ –≤ —á–∞—Å–∞—Ö
        train_max_year : int
            –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏
        val_max_year : int
            –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≥–æ–¥ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
        validate_data : bool
            –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ
        """
        self.horizons_hours = horizons_hours
        self.train_max_year = train_max_year
        self.val_max_year = val_max_year
        self.validate_data = validate_data

    def process_full_pipeline(self, df: pd.DataFrame) -> dict:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.

        Parameters
        ----------
        df : pd.DataFrame
            –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

        Returns
        -------
        dict
            –°–ª–æ–≤–∞—Ä—å —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö...")

        # 1. –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        print("üìä –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏...")
        df_processed = preprocess_cyclone_data(df)

        # 2. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        print("üéØ –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç...")
        X, y, times, intl_ids, storm_names = build_dataset(
            df_processed,
            horizons_hours=self.horizons_hours,
            validate_data=self.validate_data,
        )

        # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        print("‚úÇÔ∏è –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ...")
        X_train, y_train, X_val, y_val, X_test, y_test = split_data_by_years(
            X, y, times, train_max_year=self.train_max_year, val_max_year=self.val_max_year
        )

        # 4. –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–æ–∫ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        mask_train = times.dt.year <= self.train_max_year
        mask_val = (times.dt.year > self.train_max_year) & (times.dt.year <= self.val_max_year)
        mask_test = times.dt.year > self.val_max_year

        # 5. –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        print_dataset_info(X, y, times, X_train, X_val, X_test, mask_train, mask_val, mask_test)

        return {
            "df_processed": df_processed,
            "X": X,
            "y": y,
            "times": times,
            "intl_ids": intl_ids,
            "storm_names": storm_names,
            "X_train": X_train,
            "y_train": y_train,
            "X_val": X_val,
            "y_val": y_val,
            "X_test": X_test,
            "y_test": y_test,
            "mask_train": mask_train,
            "mask_val": mask_val,
            "mask_test": mask_test,
        }
