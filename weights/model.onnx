
pytorch2.7.1+cu126:öÁ
ö
	sequencesval_0node_Shape_0"Shape*

end†*
start†JÑ
	namespacew_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_10: aten.sym_size.intJd
pkg.torch.onnx.class_hierarchyB['_empty_nn_module_stack_from_metadata_hook', 'aten.sym_size.int']Jî
pkg.torch.onnx.fx_nodez%sym_size_int_10 : [num_users=26] = call_function[target=torch.ops.aten.sym_size.int](args = (%sequences, 1), kwargs = {})J^
pkg.torch.onnx.name_scopes@['_empty_nn_module_stack_from_metadata_hook', 'sym_size_int_10']Js
pkg.torch.onnx.stack_traceUFile "torch/fx/passes/runtime_assert.py", line 24, in insert_deferred_runtime_asserts
ä
val_0sym_size_int_10node_Squeeze_1"SqueezeJÑ
	namespacew_empty_nn_module_stack_from_metadata_hook: _empty_nn_module_stack_from_metadata_hook/sym_size_int_10: aten.sym_size.intJd
pkg.torch.onnx.class_hierarchyB['_empty_nn_module_stack_from_metadata_hook', 'aten.sym_size.int']Jî
pkg.torch.onnx.fx_nodez%sym_size_int_10 : [num_users=26] = call_function[target=torch.ops.aten.sym_size.int](args = (%sequences, 1), kwargs = {})J^
pkg.torch.onnx.name_scopes@['_empty_nn_module_stack_from_metadata_hook', 'sym_size_int_10']Js
pkg.torch.onnx.stack_traceUFile "torch/fx/passes/runtime_assert.py", line 24, in insert_deferred_runtime_asserts
§	
	sequences
val_1val_2node_MatMul_3"MatMulJ£
	namespaceï: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.input_projection: torch.nn.modules.linear.Linear/linear: aten.linear.defaultJõ
pkg.torch.onnx.class_hierarchyy['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÃ
pkg.torch.onnx.fx_node±%linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%sequences, %p_model_input_projection_weight, %p_model_input_projection_bias), kwargs = {})JO
pkg.torch.onnx.name_scopes1['', 'model', 'model.input_projection', 'linear']Jç
pkg.torch.onnx.stack_traceÓFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 137, in forward
    x = self.input_projection(sequences)  # [batch_size, seq_len, hidden_dim]
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
±	
val_2
model.input_projection.biaslinear
node_Add_4"AddJ£
	namespaceï: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.input_projection: torch.nn.modules.linear.Linear/linear: aten.linear.defaultJõ
pkg.torch.onnx.class_hierarchyy['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÃ
pkg.torch.onnx.fx_node±%linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%sequences, %p_model_input_projection_weight, %p_model_input_projection_bias), kwargs = {})JO
pkg.torch.onnx.name_scopes1['', 'model', 'model.input_projection', 'linear']Jç
pkg.torch.onnx.stack_traceÓFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 137, in forward
    x = self.input_projection(sequences)  # [batch_size, seq_len, hidden_dim]
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
ò
model.pos_encoding
val_6
val_0
val_6
val_13slice_1node_Slice_16"SliceJj
	namespace]: models.model.NNLatLon/model: models.model.SimpleTransformerModel/slice_1: aten.slice.TensorJw
pkg.torch.onnx.class_hierarchyU['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.slice.Tensor']J¨
pkg.torch.onnx.fx_nodeë%slice_1 : [num_users=1] = call_function[target=torch.ops.aten.slice.Tensor](args = (%b_model_pos_encoding, 0, 0, %sym_size_int_10), kwargs = {})J6
pkg.torch.onnx.name_scopes['', 'model', 'slice_1']J˘
pkg.torch.onnx.stack_trace⁄File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 140, in forward
    pos_enc = self.pos_encoding[:seq_len, :].unsqueeze(0).expand(batch_size, -1, -1)  # type: ignore[index]
Û
slice_1
val_6	unsqueezenode_Unsqueeze_31"	UnsqueezeJq
	namespaced: models.model.NNLatLon/model: models.model.SimpleTransformerModel/unsqueeze: aten.unsqueeze.defaultJ|
pkg.torch.onnx.class_hierarchyZ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.unsqueeze.default']Jê
pkg.torch.onnx.fx_nodev%unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%slice_2, 0), kwargs = {})J8
pkg.torch.onnx.name_scopes['', 'model', 'unsqueeze']J˘
pkg.torch.onnx.stack_trace⁄File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 140, in forward
    pos_enc = self.pos_encoding[:seq_len, :].unsqueeze(0).expand(batch_size, -1, -1)  # type: ignore[index]
Á
	unsqueeze
val_29expandnode_Expand_35"ExpandJk
	namespace^: models.model.NNLatLon/model: models.model.SimpleTransformerModel/expand: aten.expand.defaultJy
pkg.torch.onnx.class_hierarchyW['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.expand.default']Jñ
pkg.torch.onnx.fx_node|%expand : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%unsqueeze, [1, -1, -1]), kwargs = {})J5
pkg.torch.onnx.name_scopes['', 'model', 'expand']J˘
pkg.torch.onnx.stack_trace⁄File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 140, in forward
    pos_enc = self.pos_encoding[:seq_len, :].unsqueeze(0).expand(batch_size, -1, -1)  # type: ignore[index]
Û
linear
expandadd_15node_Add_36"AddJg
	namespaceZ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/add_15: aten.add.TensorJu
pkg.torch.onnx.class_hierarchyS['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.add.Tensor']Jã
pkg.torch.onnx.fx_nodeq%add_15 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%linear, %expand), kwargs = {})J5
pkg.torch.onnx.name_scopes['', 'model', 'add_15']J°
pkg.torch.onnx.stack_traceÇFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 141, in forward
    x = x + pos_enc
Ç
val_3
sym_size_int_10
val_21arangenode_Range_41"RangeJk
	namespace^: models.model.NNLatLon/model: models.model.SimpleTransformerModel/arange: aten.arange.defaultJy
pkg.torch.onnx.class_hierarchyW['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.arange.default']JØ
pkg.torch.onnx.fx_nodeî%arange : [num_users=1] = call_function[target=torch.ops.aten.arange.default](args = (%sym_size_int_10,), kwargs = {device: cpu, pin_memory: False})J5
pkg.torch.onnx.name_scopes['', 'model', 'arange']J
pkg.torch.onnx.stack_trace—File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 144, in forward
    mask = torch.arange(seq_len, device=sequences.device).unsqueeze(0) >= seq_lengths.unsqueeze(1)

arange
val_6unsqueeze_1node_Unsqueeze_42"	UnsqueezeJs
	namespacef: models.model.NNLatLon/model: models.model.SimpleTransformerModel/unsqueeze_1: aten.unsqueeze.defaultJ|
pkg.torch.onnx.class_hierarchyZ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.unsqueeze.default']Jë
pkg.torch.onnx.fx_nodew%unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arange, 0), kwargs = {})J:
pkg.torch.onnx.name_scopes['', 'model', 'unsqueeze_1']J
pkg.torch.onnx.stack_trace—File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 144, in forward
    mask = torch.arange(seq_len, device=sequences.device).unsqueeze(0) >= seq_lengths.unsqueeze(1)
˚
seq_lengths
val_13unsqueeze_2node_Unsqueeze_44"	UnsqueezeJs
	namespacef: models.model.NNLatLon/model: models.model.SimpleTransformerModel/unsqueeze_2: aten.unsqueeze.defaultJ|
pkg.torch.onnx.class_hierarchyZ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.unsqueeze.default']Jñ
pkg.torch.onnx.fx_node|%unsqueeze_2 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%seq_lengths, 1), kwargs = {})J:
pkg.torch.onnx.name_scopes['', 'model', 'unsqueeze_2']J
pkg.torch.onnx.stack_trace—File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 144, in forward
    mask = torch.arange(seq_len, device=sequences.device).unsqueeze(0) >= seq_lengths.unsqueeze(1)
Ÿ
unsqueeze_1
unsqueeze_2genode_GreaterOrEqual_45"GreaterOrEqualJb
	namespaceU: models.model.NNLatLon/model: models.model.SimpleTransformerModel/ge: aten.ge.TensorJt
pkg.torch.onnx.class_hierarchyR['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.ge.Tensor']Jê
pkg.torch.onnx.fx_nodev%ge : [num_users=2] = call_function[target=torch.ops.aten.ge.Tensor](args = (%unsqueeze_1, %unsqueeze_2), kwargs = {})J1
pkg.torch.onnx.name_scopes['', 'model', 'ge']J
pkg.torch.onnx.stack_trace—File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 144, in forward
    mask = torch.arange(seq_len, device=sequences.device).unsqueeze(0) >= seq_lengths.unsqueeze(1)
√	
geval_36node_Shape_47"Shape*
start †Jø
	namespace±: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/zeros_like: aten.zeros_like.defaultJ±
pkg.torch.onnx.class_hierarchyé['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'aten.zeros_like.default']J≥
pkg.torch.onnx.fx_nodeò%zeros_like : [num_users=1] = call_function[target=torch.ops.aten.zeros_like.default](args = (%ge,), kwargs = {dtype: torch.float32, pin_memory: False})JV
pkg.torch.onnx.name_scopes8['', 'model', 'model.transformer_encoder', 'zeros_like']Jå
pkg.torch.onnx.stack_traceÌFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 411, in forward
    src_key_padding_mask = F._canonical_mask(
«	
val_35
val_36
zeros_likenode_Expand_48"ExpandJø
	namespace±: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/zeros_like: aten.zeros_like.defaultJ±
pkg.torch.onnx.class_hierarchyé['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'aten.zeros_like.default']J≥
pkg.torch.onnx.fx_nodeò%zeros_like : [num_users=1] = call_function[target=torch.ops.aten.zeros_like.default](args = (%ge,), kwargs = {dtype: torch.float32, pin_memory: False})JV
pkg.torch.onnx.name_scopes8['', 'model', 'model.transformer_encoder', 'zeros_like']Jå
pkg.torch.onnx.stack_traceÌFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 411, in forward
    src_key_padding_mask = F._canonical_mask(
º	
ge
val_38

zeros_likemasked_fillnode_Where_51"WhereJ¿
	namespace≤: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/masked_fill: aten.masked_fill.ScalarJ±
pkg.torch.onnx.class_hierarchyé['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'aten.masked_fill.Scalar']Jü
pkg.torch.onnx.fx_nodeÑ%masked_fill : [num_users=2] = call_function[target=torch.ops.aten.masked_fill.Scalar](args = (%zeros_like, %ge, -inf), kwargs = {})JW
pkg.torch.onnx.name_scopes9['', 'model', 'model.transformer_encoder', 'masked_fill']Jå
pkg.torch.onnx.stack_traceÌFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 411, in forward
    src_key_padding_mask = F._canonical_mask(
Ó
add_15
/model.transformer_encoder.layers.0.norm1.weight
-model.transformer_encoder.layers.0.norm1.bias
layer_normnode_LayerNormalization_52"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.norm1: torch.nn.modules.normalization.LayerNorm/layer_norm: aten.layer_norm.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']J¸
pkg.torch.onnx.fx_node·%layer_norm : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_15, [128], %p_model_transformer_encoder_layers_0_norm1_weight, %p_model_transformer_encoder_layers_0_norm1_bias), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.norm1', 'layer_norm']J⁄
pkg.torch.onnx.stack_traceªFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 908, in forward
    self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
‹

layer_norm	transposenode_Transpose_53"	Transpose*
perm@@ @†JÔ
	namespace·: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose: aten.transpose.intJñ
pkg.torch.onnx.class_hierarchyÛ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%layer_norm, 1, 0), kwargs = {})J¨
pkg.torch.onnx.name_scopesç['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'transpose']Jƒ
pkg.torch.onnx.stack_trace•File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1339, in forward
    query = key = value = query.transpose(1, 0)
ÿ
	transpose
val_41val_42node_MatMul_55"MatMulJÔ
	namespace·: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_1: aten.linear.defaultJó
pkg.torch.onnx.class_hierarchyÙ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']Jä
pkg.torch.onnx.fx_nodeÔ%linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose, %p_model_transformer_encoder_layers_0_self_attn_in_proj_weight, %p_model_transformer_encoder_layers_0_self_attn_in_proj_bias), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'linear_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ñ
val_42
9model.transformer_encoder.layers.0.self_attn.in_proj_biaslinear_1node_Add_56"AddJÔ
	namespace·: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_1: aten.linear.defaultJó
pkg.torch.onnx.class_hierarchyÙ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']Jä
pkg.torch.onnx.fx_nodeÔ%linear_1 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose, %p_model_transformer_encoder_layers_0_self_attn_in_proj_weight, %p_model_transformer_encoder_layers_0_self_attn_in_proj_bias), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'linear_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ˇ
val_0
val_13
val_45
val_46val_47node_Concat_61"Concat*
axis †JÈ
	namespace€: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J§
pkg.torch.onnx.fx_nodeâ%view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_1, [%sym_size_int_10, 1, 3, 128]), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
˜
linear_1
val_47viewnode_Reshape_63"Reshape*
	allowzero†JÈ
	namespace€: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J§
pkg.torch.onnx.fx_nodeâ%view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_1, [%sym_size_int_10, 1, 3, 128]), kwargs = {})Jß
pkg.torch.onnx.name_scopesà['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ó
view
val_6unsqueeze_3node_Unsqueeze_64"	UnsqueezeJı
	namespaceÁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/unsqueeze_3: aten.unsqueeze.defaultJö
pkg.torch.onnx.class_hierarchy˜['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.unsqueeze.default']Jè
pkg.torch.onnx.fx_nodeu%unsqueeze_3 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view, 0), kwargs = {})JÆ
pkg.torch.onnx.name_scopesè['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'unsqueeze_3']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ç
unsqueeze_3transpose_1node_Transpose_65"	Transpose*
perm@@@@ @†JÒ
	namespace„: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_1: aten.transpose.intJñ
pkg.torch.onnx.class_hierarchyÛ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jñ
pkg.torch.onnx.fx_node|%transpose_1 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%unsqueeze_3, 0, -2), kwargs = {})JÆ
pkg.torch.onnx.name_scopesè['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'transpose_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ÿ
transpose_1
val_49squeezenode_Squeeze_67"SqueezeJÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/squeeze: aten.squeeze.dimJî
pkg.torch.onnx.class_hierarchyÒ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.squeeze.dim']Jç
pkg.torch.onnx.fx_nodes%squeeze : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%transpose_1, -2), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'squeeze']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
”
squeeze
val_3selectnode_Gather_69"Gather*
axis †JÈ
	namespace€: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/select: aten.select.intJì
pkg.torch.onnx.class_hierarchy['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Já
pkg.torch.onnx.fx_nodem%select : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone, 0, 0), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'select']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
‹
squeeze
val_21select_1node_Gather_70"Gather*
axis †JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/select_1: aten.select.intJì
pkg.torch.onnx.class_hierarchy['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jâ
pkg.torch.onnx.fx_nodeo%select_1 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone, 0, 1), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'select_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
‹
squeeze
val_50select_2node_Gather_72"Gather*
axis †JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/select_2: aten.select.intJì
pkg.torch.onnx.class_hierarchy['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jâ
pkg.torch.onnx.fx_nodeo%select_2 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone, 0, 2), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'select_2']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
˜
val_0
val_53
val_54val_55node_Concat_77"Concat*
axis †JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_1: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J†
pkg.torch.onnx.fx_nodeÖ%view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select, [%sym_size_int_10, 8, 16]), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
˜
select
val_55view_1node_Reshape_79"Reshape*
	allowzero†JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_1: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J†
pkg.torch.onnx.fx_nodeÖ%view_1 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select, [%sym_size_int_10, 8, 16]), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Û
view_1transpose_2node_Transpose_80"	Transpose*
perm@@ @†JÒ
	namespace„: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_2: aten.transpose.intJñ
pkg.torch.onnx.class_hierarchyÛ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jê
pkg.torch.onnx.fx_nodev%transpose_2 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_1, 0, 1), kwargs = {})JÆ
pkg.torch.onnx.name_scopesè['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'transpose_2']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
˚
select_1
val_55view_2node_Reshape_85"Reshape*
	allowzero†JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_2: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J¢
pkg.torch.onnx.fx_nodeá%view_2 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_1, [%sym_size_int_10, 8, 16]), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_2']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Û
view_2transpose_3node_Transpose_86"	Transpose*
perm@@ @†JÒ
	namespace„: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_3: aten.transpose.intJñ
pkg.torch.onnx.class_hierarchyÛ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jê
pkg.torch.onnx.fx_nodev%transpose_3 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_2, 0, 1), kwargs = {})JÆ
pkg.torch.onnx.name_scopesè['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'transpose_3']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
˚
select_2
val_55view_3node_Reshape_91"Reshape*
	allowzero†JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_3: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J¢
pkg.torch.onnx.fx_nodeá%view_3 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_2, [%sym_size_int_10, 8, 16]), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_3']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Û
view_3transpose_4node_Transpose_92"	Transpose*
perm@@ @†JÒ
	namespace„: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_4: aten.transpose.intJñ
pkg.torch.onnx.class_hierarchyÛ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jê
pkg.torch.onnx.fx_nodev%transpose_4 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_3, 0, 1), kwargs = {})JÆ
pkg.torch.onnx.name_scopesè['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'transpose_4']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
á
val_13
val_53
val_0
val_54val_67node_Concat_95"Concat*
axis †JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_7: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J®
pkg.torch.onnx.fx_nodeç%view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_2, [1, 8, %sym_size_int_10, 16]), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_7']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ñ
transpose_2
val_67view_7node_Reshape_97"Reshape*
	allowzero†JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_7: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J®
pkg.torch.onnx.fx_nodeç%view_7 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_2, [1, 8, %sym_size_int_10, 16]), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_7']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ö
transpose_3
val_67view_8node_Reshape_102"Reshape*
	allowzero†JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_8: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J®
pkg.torch.onnx.fx_nodeç%view_8 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_3, [1, 8, %sym_size_int_10, 16]), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_8']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ö
transpose_4
val_67view_9node_Reshape_107"Reshape*
	allowzero†JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_9: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J®
pkg.torch.onnx.fx_nodeç%view_9 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_4, [1, 8, %sym_size_int_10, 16]), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_9']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ä
val_13
val_13
val_13
val_0val_79node_Concat_110"Concat*
axis †JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_10: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J®
pkg.torch.onnx.fx_nodeç%view_10 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%masked_fill, [1, 1, 1, %sym_size_int_10]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_10']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
à
masked_fill
val_79view_10node_Reshape_112"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_10: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J®
pkg.torch.onnx.fx_nodeç%view_10 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%masked_fill, [1, 1, 1, %sym_size_int_10]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_10']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Í
view_10
val_83expand_2node_Expand_116"ExpandJÔ
	namespace·: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/expand_2: aten.expand.defaultJó
pkg.torch.onnx.class_hierarchyÙ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.expand.default']Jõ
pkg.torch.onnx.fx_nodeÄ%expand_2 : [num_users=1] = call_function[target=torch.ops.aten.expand.default](args = (%view_10, [-1, 8, -1, -1]), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'expand_2']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¸
val_53
val_13
val_0val_86node_Concat_119"Concat*
axis †JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_11: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J¢
pkg.torch.onnx.fx_nodeá%view_11 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%expand_2, [8, 1, %sym_size_int_10]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_11']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ˇ
expand_2
val_86view_11node_Reshape_121"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_11: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J¢
pkg.torch.onnx.fx_nodeá%view_11 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%expand_2, [8, 1, %sym_size_int_10]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_11']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
á
val_13
val_53
val_88
val_0val_91node_Concat_125"Concat*
axis †JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_12: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J•
pkg.torch.onnx.fx_nodeä%view_12 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%view_11, [1, 8, -1, %sym_size_int_10]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_12']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Å
view_11
val_91view_12node_Reshape_127"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_12: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J•
pkg.torch.onnx.fx_nodeä%view_12 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%view_11, [1, 8, -1, %sym_size_int_10]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_12']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ù
view_8val_102node_Shape_137"Shape*
start †Jô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¯
val_102
val_88
val_103val_104node_Slice_139"SliceJô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
˜
val_102
val_49
val_88val_105node_Slice_140"SliceJô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
¯
val_102
val_106
val_49val_107node_Slice_142"SliceJô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
á
val_88
val_105
val_104val_109node_Concat_144"Concat*
axis †Jô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ö
view_8
val_109val_110node_Reshape_145"Reshape*
	allowzero †Jô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ä
val_110val_111node_Transpose_146"	Transpose*
perm@ @@†Jô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
à
val_107
val_104
val_105val_112node_Concat_147"Concat*
axis †Jô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ü
val_111
val_112val_113node_Reshape_148"Reshape*
	allowzero †Jô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Î
view_7
val_114val_115node_Mul_150"MulJô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ï
val_113
val_114val_117node_Mul_152"MulJô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ú
val_115
val_117val_118node_MatMul_153"MatMulJô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ï
val_118
view_12val_119node_Add_154"AddJô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Å
val_119val_120node_Softmax_155"Softmax*
axisˇˇˇˇˇˇˇˇˇ†Jô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ü
val_120
view_9scaled_dot_product_attentionnode_MatMul_157"MatMulJô
	namespaceã: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']Jœ
pkg.torch.onnx.fx_node¥%scaled_dot_product_attention : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_7, %view_8, %view_9, %view_12), kwargs = {})Jø
pkg.torch.onnx.name_scopes†['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'scaled_dot_product_attention']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
°
scaled_dot_product_attentionpermutenode_Transpose_158"	Transpose*
perm@@ @@†JÔ
	namespace·: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/permute: aten.permute.defaultJò
pkg.torch.onnx.class_hierarchyı['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.permute.default']J≠
pkg.torch.onnx.fx_nodeí%permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%scaled_dot_product_attention, [2, 0, 1, 3]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'permute']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Û
val_0
val_46val_125node_Concat_161"Concat*
axis †JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_13: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J†
pkg.torch.onnx.fx_nodeÖ%view_13 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute, [%sym_size_int_10, 128]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_13']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
˝
permute
val_125view_13node_Reshape_163"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_13: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J†
pkg.torch.onnx.fx_nodeÖ%view_13 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute, [%sym_size_int_10, 128]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_13']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ü
view_13
<model.transformer_encoder.layers.0.self_attn.out_proj.weight
:model.transformer_encoder.layers.0.self_attn.out_proj.biaslinear_2node_Gemm_164"Gemm*
beta  Ä?†*
transB†*
alpha  Ä?†*
transA †JÔ
	namespace·: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_2: aten.linear.defaultJó
pkg.torch.onnx.class_hierarchyÙ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']Jä
pkg.torch.onnx.fx_nodeÔ%linear_2 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_13, %p_model_transformer_encoder_layers_0_self_attn_out_proj_weight, %p_model_transformer_encoder_layers_0_self_attn_out_proj_bias), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'linear_2']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ˇ
val_0
val_13
val_46val_129node_Concat_167"Concat*
axis †JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_14: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J§
pkg.torch.onnx.fx_nodeâ%view_14 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_2, [%sym_size_int_10, 1, 128]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_14']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ç
linear_2
val_129view_14node_Reshape_169"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/view_14: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J§
pkg.torch.onnx.fx_nodeâ%view_14 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_2, [%sym_size_int_10, 1, 128]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'view_14']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Î
view_14transpose_5node_Transpose_170"	Transpose*
perm@@ @†JÒ
	namespace„: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_5: aten.transpose.intJñ
pkg.torch.onnx.class_hierarchyÛ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jë
pkg.torch.onnx.fx_nodew%transpose_5 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_14, 1, 0), kwargs = {})JÆ
pkg.torch.onnx.name_scopesè['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.self_attn', 'transpose_5']J–
pkg.torch.onnx.stack_trace±File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1395, in forward
    return attn_output.transpose(1, 0), attn_output_weights
√
add_15
transpose_5add_142node_Add_172"AddJç
	namespaceˇ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/add_142: aten.add.TensorJ·
pkg.torch.onnx.class_hierarchyæ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.add.Tensor']Jç
pkg.torch.onnx.fx_nodes%add_142 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_15, %clone_1), kwargs = {})Jy
pkg.torch.onnx.name_scopes[['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'add_142']Jê
pkg.torch.onnx.stack_traceÒFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
‹
add_142
/model.transformer_encoder.layers.0.norm2.weight
-model.transformer_encoder.layers.0.norm2.biaslayer_norm_1node_LayerNormalization_173"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÌ
	namespaceﬂ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.norm2: torch.nn.modules.normalization.LayerNorm/layer_norm_1: aten.layer_norm.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']Jˇ
pkg.torch.onnx.fx_node‰%layer_norm_1 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_142, [128], %p_model_transformer_encoder_layers_0_norm2_weight, %p_model_transformer_encoder_layers_0_norm2_bias), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.norm2', 'layer_norm_1']JΩ
pkg.torch.onnx.stack_traceûFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
û
layer_norm_1
val_133val_134node_MatMul_175"MatMulJ›
	namespaceœ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.linear1: torch.nn.modules.linear.Linear/linear_3: aten.linear.defaultJá
pkg.torch.onnx.class_hierarchy‰['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J˘
pkg.torch.onnx.fx_nodeﬁ%linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_1, %p_model_transformer_encoder_layers_0_linear1_weight, %p_model_transformer_encoder_layers_0_linear1_bias), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.linear1', 'linear_3']J–
pkg.torch.onnx.stack_trace±File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
º
val_134
/model.transformer_encoder.layers.0.linear1.biaslinear_3node_Add_176"AddJ›
	namespaceœ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.linear1: torch.nn.modules.linear.Linear/linear_3: aten.linear.defaultJá
pkg.torch.onnx.class_hierarchy‰['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J˘
pkg.torch.onnx.fx_nodeﬁ%linear_3 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_1, %p_model_transformer_encoder_layers_0_linear1_weight, %p_model_transformer_encoder_layers_0_linear1_bias), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.linear1', 'linear_3']J–
pkg.torch.onnx.stack_trace±File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
ª
linear_3relunode_Relu_177"ReluJå
	namespace˛: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/relu: aten.relu.defaultJ„
pkg.torch.onnx.class_hierarchy¿['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.relu.default']JÖ
pkg.torch.onnx.fx_nodek%relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%linear_3,), kwargs = {})Jv
pkg.torch.onnx.name_scopesX['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'relu']Jû
pkg.torch.onnx.stack_traceˇFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
ë
relu
val_135val_136node_MatMul_180"MatMulJ›
	namespaceœ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.linear2: torch.nn.modules.linear.Linear/linear_4: aten.linear.defaultJá
pkg.torch.onnx.class_hierarchy‰['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÙ
pkg.torch.onnx.fx_nodeŸ%linear_4 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_2, %p_model_transformer_encoder_layers_0_linear2_weight, %p_model_transformer_encoder_layers_0_linear2_bias), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.linear2', 'linear_4']J–
pkg.torch.onnx.stack_trace±File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
∑
val_136
/model.transformer_encoder.layers.0.linear2.biaslinear_4node_Add_181"AddJ›
	namespaceœ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.0.linear2: torch.nn.modules.linear.Linear/linear_4: aten.linear.defaultJá
pkg.torch.onnx.class_hierarchy‰['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÙ
pkg.torch.onnx.fx_nodeŸ%linear_4 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_2, %p_model_transformer_encoder_layers_0_linear2_weight, %p_model_transformer_encoder_layers_0_linear2_bias), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'model.transformer_encoder.layers.0.linear2', 'linear_4']J–
pkg.torch.onnx.stack_trace±File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
–
add_142
linear_4add_164node_Add_183"AddJç
	namespaceˇ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.0: torch.nn.modules.transformer.TransformerEncoderLayer/add_164: aten.add.TensorJ·
pkg.torch.onnx.class_hierarchyæ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_164 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_142, %clone_3), kwargs = {})Jy
pkg.torch.onnx.name_scopes[['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.0', 'add_164']Jû
pkg.torch.onnx.stack_traceˇFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
˘
add_164
/model.transformer_encoder.layers.1.norm1.weight
-model.transformer_encoder.layers.1.norm1.biaslayer_norm_2node_LayerNormalization_184"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÌ
	namespaceﬂ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.norm1: torch.nn.modules.normalization.LayerNorm/layer_norm_2: aten.layer_norm.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']Jˇ
pkg.torch.onnx.fx_node‰%layer_norm_2 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_164, [128], %p_model_transformer_encoder_layers_1_norm1_weight, %p_model_transformer_encoder_layers_1_norm1_bias), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.norm1', 'layer_norm_2']J⁄
pkg.torch.onnx.stack_traceªFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 908, in forward
    self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
È
layer_norm_2transpose_6node_Transpose_185"	Transpose*
perm@@ @†JÒ
	namespace„: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_6: aten.transpose.intJñ
pkg.torch.onnx.class_hierarchyÛ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jñ
pkg.torch.onnx.fx_node|%transpose_6 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%layer_norm_2, 1, 0), kwargs = {})JÆ
pkg.torch.onnx.name_scopesè['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'transpose_6']Jƒ
pkg.torch.onnx.stack_trace•File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1339, in forward
    query = key = value = query.transpose(1, 0)
ﬂ
transpose_6
val_139val_140node_MatMul_187"MatMulJÔ
	namespace·: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_5: aten.linear.defaultJó
pkg.torch.onnx.class_hierarchyÙ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']Jå
pkg.torch.onnx.fx_nodeÒ%linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_6, %p_model_transformer_encoder_layers_1_self_attn_in_proj_weight, %p_model_transformer_encoder_layers_1_self_attn_in_proj_bias), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'linear_5']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
à
val_140
9model.transformer_encoder.layers.1.self_attn.in_proj_biaslinear_5node_Add_188"AddJÔ
	namespace·: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_5: aten.linear.defaultJó
pkg.torch.onnx.class_hierarchyÙ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']Jå
pkg.torch.onnx.fx_nodeÒ%linear_5 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%transpose_6, %p_model_transformer_encoder_layers_1_self_attn_in_proj_weight, %p_model_transformer_encoder_layers_1_self_attn_in_proj_bias), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'linear_5']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ñ
linear_5
val_47view_15node_Reshape_193"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_15: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']Jß
pkg.torch.onnx.fx_nodeå%view_15 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_5, [%sym_size_int_10, 1, 3, 128]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'view_15']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ı
view_15
val_6unsqueeze_4node_Unsqueeze_194"	UnsqueezeJı
	namespaceÁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/unsqueeze_4: aten.unsqueeze.defaultJö
pkg.torch.onnx.class_hierarchy˜['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.unsqueeze.default']Jí
pkg.torch.onnx.fx_nodex%unsqueeze_4 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%view_15, 0), kwargs = {})JÆ
pkg.torch.onnx.name_scopesè['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'unsqueeze_4']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
É
unsqueeze_4transpose_7node_Transpose_195"	Transpose*
perm@@@@ @†JÒ
	namespace„: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_7: aten.transpose.intJñ
pkg.torch.onnx.class_hierarchyÛ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jñ
pkg.torch.onnx.fx_node|%transpose_7 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%unsqueeze_4, 0, -2), kwargs = {})JÆ
pkg.torch.onnx.name_scopesè['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'transpose_7']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
·
transpose_7
val_49	squeeze_1node_Squeeze_196"SqueezeJÌ
	namespaceﬂ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/squeeze_1: aten.squeeze.dimJî
pkg.torch.onnx.class_hierarchyÒ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.squeeze.dim']Jè
pkg.torch.onnx.fx_nodeu%squeeze_1 : [num_users=1] = call_function[target=torch.ops.aten.squeeze.dim](args = (%transpose_7, -2), kwargs = {})J¨
pkg.torch.onnx.name_scopesç['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'squeeze_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
‡
	squeeze_1
val_3select_3node_Gather_198"Gather*
axis †JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/select_3: aten.select.intJì
pkg.torch.onnx.class_hierarchy['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jã
pkg.torch.onnx.fx_nodeq%select_3 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_4, 0, 0), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'select_3']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
·
	squeeze_1
val_21select_4node_Gather_199"Gather*
axis †JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/select_4: aten.select.intJì
pkg.torch.onnx.class_hierarchy['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jã
pkg.torch.onnx.fx_nodeq%select_4 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_4, 0, 1), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'select_4']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
·
	squeeze_1
val_50select_5node_Gather_200"Gather*
axis †JÎ
	namespace›: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/select_5: aten.select.intJì
pkg.torch.onnx.class_hierarchy['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.select.int']Jã
pkg.torch.onnx.fx_nodeq%select_5 : [num_users=1] = call_function[target=torch.ops.aten.select.int](args = (%clone_4, 0, 2), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'select_5']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ä
select_3
val_55view_16node_Reshape_205"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_16: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J£
pkg.torch.onnx.fx_nodeà%view_16 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_3, [%sym_size_int_10, 8, 16]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'view_16']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ˆ
view_16transpose_8node_Transpose_206"	Transpose*
perm@@ @†JÒ
	namespace„: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_8: aten.transpose.intJñ
pkg.torch.onnx.class_hierarchyÛ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jë
pkg.torch.onnx.fx_nodew%transpose_8 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_16, 0, 1), kwargs = {})JÆ
pkg.torch.onnx.name_scopesè['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'transpose_8']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ä
select_4
val_55view_17node_Reshape_211"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_17: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J£
pkg.torch.onnx.fx_nodeà%view_17 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_4, [%sym_size_int_10, 8, 16]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'view_17']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ˆ
view_17transpose_9node_Transpose_212"	Transpose*
perm@@ @†JÒ
	namespace„: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_9: aten.transpose.intJñ
pkg.torch.onnx.class_hierarchyÛ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jë
pkg.torch.onnx.fx_nodew%transpose_9 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_17, 0, 1), kwargs = {})JÆ
pkg.torch.onnx.name_scopesè['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'transpose_9']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ä
select_5
val_55view_18node_Reshape_217"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_18: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J£
pkg.torch.onnx.fx_nodeà%view_18 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%select_5, [%sym_size_int_10, 8, 16]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'view_18']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
˙
view_18transpose_10node_Transpose_218"	Transpose*
perm@@ @†JÚ
	namespace‰: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_10: aten.transpose.intJñ
pkg.torch.onnx.class_hierarchyÛ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_10 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_18, 0, 1), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'transpose_10']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
â
transpose_8
val_67view_22node_Reshape_223"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_22: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J©
pkg.torch.onnx.fx_nodeé%view_22 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_8, [1, 8, %sym_size_int_10, 16]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'view_22']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
â
transpose_9
val_67view_23node_Reshape_228"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_23: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J©
pkg.torch.onnx.fx_nodeé%view_23 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_9, [1, 8, %sym_size_int_10, 16]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'view_23']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ã
transpose_10
val_67view_24node_Reshape_233"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_24: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J™
pkg.torch.onnx.fx_nodeè%view_24 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%transpose_10, [1, 8, %sym_size_int_10, 16]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'view_24']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
˛
view_23val_192node_Shape_261"Shape*
start †Jõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Å
val_192
val_88
val_103val_194node_Slice_263"SliceJõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ä
val_192
val_49
val_88val_195node_Slice_264"SliceJõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Å
val_192
val_106
val_49val_197node_Slice_266"SliceJõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ê
val_88
val_195
val_194val_199node_Concat_268"Concat*
axis †Jõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
è
view_23
val_199val_200node_Reshape_269"Reshape*
	allowzero †Jõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
â
val_200val_201node_Transpose_270"	Transpose*
perm@ @@†Jõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ë
val_197
val_194
val_195val_202node_Concat_271"Concat*
axis †Jõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
è
val_201
val_202val_203node_Reshape_272"Reshape*
	allowzero †Jõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ı
view_22
val_114val_205node_Mul_274"MulJõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ı
val_203
val_114val_207node_Mul_276"MulJõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
˚
val_205
val_207val_208node_MatMul_277"MatMulJõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ı
val_208
view_12val_209node_Add_278"AddJõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
ä
val_209val_210node_Softmax_279"Softmax*
axisˇˇˇˇˇˇˇˇˇ†Jõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
í
val_210
view_24scaled_dot_product_attention_1node_MatMul_281"MatMulJõ
	namespaceç: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/scaled_dot_product_attention_1: aten.scaled_dot_product_attention.defaultJ≠
pkg.torch.onnx.class_hierarchyä['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.scaled_dot_product_attention.default']J‘
pkg.torch.onnx.fx_nodeπ%scaled_dot_product_attention_1 : [num_users=1] = call_function[target=torch.ops.aten.scaled_dot_product_attention.default](args = (%view_22, %view_23, %view_24, %view_27), kwargs = {})J¡
pkg.torch.onnx.name_scopes¢['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'scaled_dot_product_attention_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
≠
scaled_dot_product_attention_1	permute_1node_Transpose_282"	Transpose*
perm@@ @@†JÒ
	namespace„: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/permute_1: aten.permute.defaultJò
pkg.torch.onnx.class_hierarchyı['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.permute.default']J±
pkg.torch.onnx.fx_nodeñ%permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%scaled_dot_product_attention_1, [2, 0, 1, 3]), kwargs = {})J¨
pkg.torch.onnx.name_scopesç['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'permute_1']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Å
	permute_1
val_125view_28node_Reshape_287"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_28: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J¢
pkg.torch.onnx.fx_nodeá%view_28 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%permute_1, [%sym_size_int_10, 128]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'view_28']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ü
view_28
<model.transformer_encoder.layers.1.self_attn.out_proj.weight
:model.transformer_encoder.layers.1.self_attn.out_proj.biaslinear_6node_Gemm_288"Gemm*
beta  Ä?†*
transB†*
alpha  Ä?†*
transA †JÔ
	namespace·: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/linear_6: aten.linear.defaultJó
pkg.torch.onnx.class_hierarchyÙ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.linear.default']Jä
pkg.torch.onnx.fx_nodeÔ%linear_6 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view_28, %p_model_transformer_encoder_layers_1_self_attn_out_proj_weight, %p_model_transformer_encoder_layers_1_self_attn_out_proj_bias), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'linear_6']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ç
linear_6
val_129view_29node_Reshape_293"Reshape*
	allowzero†JÏ
	namespaceﬁ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/view_29: aten.view.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.view.default']J§
pkg.torch.onnx.fx_nodeâ%view_29 : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%linear_6, [%sym_size_int_10, 1, 128]), kwargs = {})J™
pkg.torch.onnx.name_scopesã['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'view_29']J€
pkg.torch.onnx.stack_traceºFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
Ô
view_29transpose_11node_Transpose_294"	Transpose*
perm@@ @†JÚ
	namespace‰: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.self_attn: torch.nn.modules.activation.MultiheadAttention/transpose_11: aten.transpose.intJñ
pkg.torch.onnx.class_hierarchyÛ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.activation.MultiheadAttention', 'aten.transpose.int']Jí
pkg.torch.onnx.fx_nodex%transpose_11 : [num_users=1] = call_function[target=torch.ops.aten.transpose.int](args = (%view_29, 1, 0), kwargs = {})JØ
pkg.torch.onnx.name_scopesê['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.self_attn', 'transpose_11']J–
pkg.torch.onnx.stack_trace±File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 1395, in forward
    return attn_output.transpose(1, 0), attn_output_weights
∆
add_164
transpose_11add_277node_Add_296"AddJç
	namespaceˇ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/add_277: aten.add.TensorJ·
pkg.torch.onnx.class_hierarchyæ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_277 : [num_users=2] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_164, %clone_5), kwargs = {})Jy
pkg.torch.onnx.name_scopes[['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'add_277']Jê
pkg.torch.onnx.stack_traceÒFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
‹
add_277
/model.transformer_encoder.layers.1.norm2.weight
-model.transformer_encoder.layers.1.norm2.biaslayer_norm_3node_LayerNormalization_297"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÌ
	namespaceﬂ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.norm2: torch.nn.modules.normalization.LayerNorm/layer_norm_3: aten.layer_norm.defaultJï
pkg.torch.onnx.class_hierarchyÚ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']Jˇ
pkg.torch.onnx.fx_node‰%layer_norm_3 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_277, [128], %p_model_transformer_encoder_layers_1_norm2_weight, %p_model_transformer_encoder_layers_1_norm2_bias), kwargs = {})J´
pkg.torch.onnx.name_scopeså['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.norm2', 'layer_norm_3']JΩ
pkg.torch.onnx.stack_traceûFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
û
layer_norm_3
val_223val_224node_MatMul_299"MatMulJ›
	namespaceœ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.linear1: torch.nn.modules.linear.Linear/linear_7: aten.linear.defaultJá
pkg.torch.onnx.class_hierarchy‰['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J˘
pkg.torch.onnx.fx_nodeﬁ%linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_3, %p_model_transformer_encoder_layers_1_linear1_weight, %p_model_transformer_encoder_layers_1_linear1_bias), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.linear1', 'linear_7']J–
pkg.torch.onnx.stack_trace±File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
º
val_224
/model.transformer_encoder.layers.1.linear1.biaslinear_7node_Add_300"AddJ›
	namespaceœ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.linear1: torch.nn.modules.linear.Linear/linear_7: aten.linear.defaultJá
pkg.torch.onnx.class_hierarchy‰['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J˘
pkg.torch.onnx.fx_nodeﬁ%linear_7 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%layer_norm_3, %p_model_transformer_encoder_layers_1_linear1_weight, %p_model_transformer_encoder_layers_1_linear1_bias), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.linear1', 'linear_7']J–
pkg.torch.onnx.stack_trace±File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
√
linear_7relu_1node_Relu_301"ReluJé
	namespaceÄ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/relu_1: aten.relu.defaultJ„
pkg.torch.onnx.class_hierarchy¿['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.relu.default']Já
pkg.torch.onnx.fx_nodem%relu_1 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%linear_7,), kwargs = {})Jx
pkg.torch.onnx.name_scopesZ['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'relu_1']Jû
pkg.torch.onnx.stack_traceˇFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
ì
relu_1
val_225val_226node_MatMul_304"MatMulJ›
	namespaceœ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.linear2: torch.nn.modules.linear.Linear/linear_8: aten.linear.defaultJá
pkg.torch.onnx.class_hierarchy‰['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÙ
pkg.torch.onnx.fx_nodeŸ%linear_8 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_6, %p_model_transformer_encoder_layers_1_linear2_weight, %p_model_transformer_encoder_layers_1_linear2_bias), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.linear2', 'linear_8']J–
pkg.torch.onnx.stack_trace±File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
∑
val_226
/model.transformer_encoder.layers.1.linear2.biaslinear_8node_Add_305"AddJ›
	namespaceœ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/model.transformer_encoder.layers.1.linear2: torch.nn.modules.linear.Linear/linear_8: aten.linear.defaultJá
pkg.torch.onnx.class_hierarchy‰['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JÙ
pkg.torch.onnx.fx_nodeŸ%linear_8 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_6, %p_model_transformer_encoder_layers_1_linear2_weight, %p_model_transformer_encoder_layers_1_linear2_bias), kwargs = {})J©
pkg.torch.onnx.name_scopesä['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'model.transformer_encoder.layers.1.linear2', 'linear_8']J–
pkg.torch.onnx.stack_trace±File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
–
add_277
linear_8add_299node_Add_307"AddJç
	namespaceˇ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.transformer_encoder: torch.nn.modules.transformer.TransformerEncoder/model.transformer_encoder.layers.1: torch.nn.modules.transformer.TransformerEncoderLayer/add_299: aten.add.TensorJ·
pkg.torch.onnx.class_hierarchyæ['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.transformer.TransformerEncoder', 'torch.nn.modules.transformer.TransformerEncoderLayer', 'aten.add.Tensor']Jé
pkg.torch.onnx.fx_nodet%add_299 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%add_277, %clone_7), kwargs = {})Jy
pkg.torch.onnx.name_scopes[['', 'model', 'model.transformer_encoder', 'model.transformer_encoder.layers.1', 'add_299']Jû
pkg.torch.onnx.stack_traceˇFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 147, in forward
    transformer_out = self.transformer_encoder(x, src_key_padding_mask=mask)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
Î

add_299
"model.post_transformer_norm.weight
 model.post_transformer_norm.biaslayer_norm_4node_LayerNormalization_308"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†Jº
	namespaceÆ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.post_transformer_norm: torch.nn.modules.normalization.LayerNorm/layer_norm_4: aten.layer_norm.defaultJ™
pkg.torch.onnx.class_hierarchyá['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']JÂ
pkg.torch.onnx.fx_node %layer_norm_4 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%add_299, [128], %p_model_post_transformer_norm_weight, %p_model_post_transformer_norm_bias), kwargs = {})JZ
pkg.torch.onnx.name_scopes<['', 'model', 'model.post_transformer_norm', 'layer_norm_4']JÓ
pkg.torch.onnx.stack_traceœFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 150, in forward
    transformer_out = self.post_transformer_norm(transformer_out)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
¶
seq_lengths
val_21sub_92node_Sub_312"SubJg
	namespaceZ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/sub_92: aten.sub.TensorJu
pkg.torch.onnx.class_hierarchyS['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.sub.Tensor']Jä
pkg.torch.onnx.fx_nodep%sub_92 : [num_users=1] = call_function[target=torch.ops.aten.sub.Tensor](args = (%seq_lengths, 1), kwargs = {})J5
pkg.torch.onnx.name_scopes['', 'model', 'sub_92']Jœ
pkg.torch.onnx.stack_trace∞File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 154, in forward
    sequence_features = transformer_out[indices, seq_lengths - 1]
∆
sub_92
val_88val_237node_Unsqueeze_319"	UnsqueezeJh
	namespace[: models.model.NNLatLon/model: models.model.SimpleTransformerModel/index: aten.index.TensorJw
pkg.torch.onnx.class_hierarchyU['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.index.Tensor']J†
pkg.torch.onnx.fx_nodeÖ%index : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor](args = (%layer_norm_4, [%arange_1, %sub_92]), kwargs = {})J4
pkg.torch.onnx.name_scopes['', 'model', 'index']Jœ
pkg.torch.onnx.stack_trace∞File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 154, in forward
    sequence_features = transformer_out[indices, seq_lengths - 1]
ÿ
val_235
val_237val_238node_Concat_320"Concat*
axisˇˇˇˇˇˇˇˇˇ†Jh
	namespace[: models.model.NNLatLon/model: models.model.SimpleTransformerModel/index: aten.index.TensorJw
pkg.torch.onnx.class_hierarchyU['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.index.Tensor']J†
pkg.torch.onnx.fx_nodeÖ%index : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor](args = (%layer_norm_4, [%arange_1, %sub_92]), kwargs = {})J4
pkg.torch.onnx.name_scopes['', 'model', 'index']Jœ
pkg.torch.onnx.stack_trace∞File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 154, in forward
    sequence_features = transformer_out[indices, seq_lengths - 1]
ﬁ
layer_norm_4
val_238val_239node_GatherND_321"GatherND*

batch_dims †Jh
	namespace[: models.model.NNLatLon/model: models.model.SimpleTransformerModel/index: aten.index.TensorJw
pkg.torch.onnx.class_hierarchyU['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.index.Tensor']J†
pkg.torch.onnx.fx_nodeÖ%index : [num_users=1] = call_function[target=torch.ops.aten.index.Tensor](args = (%layer_norm_4, [%arange_1, %sub_92]), kwargs = {})J4
pkg.torch.onnx.name_scopes['', 'model', 'index']Jœ
pkg.torch.onnx.stack_trace∞File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 154, in forward
    sequence_features = transformer_out[indices, seq_lengths - 1]
ó
static_features
model.static_head.0.weight
model.static_head.0.biaslinear_9node_Gemm_324"Gemm*
beta  Ä?†*
transB†*
alpha  Ä?†*
transA †J€
	namespaceÕ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.static_head: torch.nn.modules.container.Sequential/model.static_head.0: torch.nn.modules.linear.Linear/linear_9: aten.linear.defaultJ≈
pkg.torch.onnx.class_hierarchy¢['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.linear.Linear', 'aten.linear.default']JŒ
pkg.torch.onnx.fx_node≥%linear_9 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%static_features, %p_model_static_head_0_weight, %p_model_static_head_0_bias), kwargs = {})Jc
pkg.torch.onnx.name_scopesE['', 'model', 'model.static_head', 'model.static_head.0', 'linear_9']Jì
pkg.torch.onnx.stack_traceÙFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 160, in forward
    static_features = self.static_head(static_features)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
≈
linear_9
model.static_head.1.weight
model.static_head.1.biaslayer_norm_5node_LayerNormalization_325"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÌ
	namespaceﬂ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.static_head: torch.nn.modules.container.Sequential/model.static_head.1: torch.nn.modules.normalization.LayerNorm/layer_norm_5: aten.layer_norm.defaultJ”
pkg.torch.onnx.class_hierarchy∞['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']J’
pkg.torch.onnx.fx_node∫%layer_norm_5 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_9, [64], %p_model_static_head_1_weight, %p_model_static_head_1_bias), kwargs = {})Jg
pkg.torch.onnx.name_scopesI['', 'model', 'model.static_head', 'model.static_head.1', 'layer_norm_5']JÄ
pkg.torch.onnx.stack_trace·File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 160, in forward
    static_features = self.static_head(static_features)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
÷

layer_norm_5relu_2node_Relu_326"ReluJŸ
	namespaceÀ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.static_head: torch.nn.modules.container.Sequential/model.static_head.2: torch.nn.modules.activation.ReLU/relu_2: aten.relu.defaultJ≈
pkg.torch.onnx.class_hierarchy¢['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.activation.ReLU', 'aten.relu.default']Jã
pkg.torch.onnx.fx_nodeq%relu_2 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%layer_norm_5,), kwargs = {})Ja
pkg.torch.onnx.name_scopesC['', 'model', 'model.static_head', 'model.static_head.2', 'relu_2']Jì
pkg.torch.onnx.stack_traceÙFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 160, in forward
    static_features = self.static_head(static_features)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 133, in forward
    return F.relu(input, inplace=self.inplace)
ä
relu_2
model.static_head.4.weight
model.static_head.4.bias	linear_10node_Gemm_328"Gemm*
beta  Ä?†*
transB†*
alpha  Ä?†*
transA †J‹
	namespaceŒ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.static_head: torch.nn.modules.container.Sequential/model.static_head.4: torch.nn.modules.linear.Linear/linear_10: aten.linear.defaultJ≈
pkg.torch.onnx.class_hierarchy¢['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J«
pkg.torch.onnx.fx_node¨%linear_10 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_9, %p_model_static_head_4_weight, %p_model_static_head_4_bias), kwargs = {})Jd
pkg.torch.onnx.name_scopesF['', 'model', 'model.static_head', 'model.static_head.4', 'linear_10']Jì
pkg.torch.onnx.stack_traceÙFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 160, in forward
    static_features = self.static_head(static_features)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
«
	linear_10
model.static_head.5.weight
model.static_head.5.biaslayer_norm_6node_LayerNormalization_329"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÌ
	namespaceﬂ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.static_head: torch.nn.modules.container.Sequential/model.static_head.5: torch.nn.modules.normalization.LayerNorm/layer_norm_6: aten.layer_norm.defaultJ”
pkg.torch.onnx.class_hierarchy∞['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']J÷
pkg.torch.onnx.fx_nodeª%layer_norm_6 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_10, [64], %p_model_static_head_5_weight, %p_model_static_head_5_bias), kwargs = {})Jg
pkg.torch.onnx.name_scopesI['', 'model', 'model.static_head', 'model.static_head.5', 'layer_norm_6']JÄ
pkg.torch.onnx.stack_trace·File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 160, in forward
    static_features = self.static_head(static_features)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
÷

layer_norm_6relu_3node_Relu_330"ReluJŸ
	namespaceÀ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.static_head: torch.nn.modules.container.Sequential/model.static_head.6: torch.nn.modules.activation.ReLU/relu_3: aten.relu.defaultJ≈
pkg.torch.onnx.class_hierarchy¢['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.activation.ReLU', 'aten.relu.default']Jã
pkg.torch.onnx.fx_nodeq%relu_3 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%layer_norm_6,), kwargs = {})Ja
pkg.torch.onnx.name_scopesC['', 'model', 'model.static_head', 'model.static_head.6', 'relu_3']Jì
pkg.torch.onnx.stack_traceÙFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 160, in forward
    static_features = self.static_head(static_features)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 133, in forward
    return F.relu(input, inplace=self.inplace)
¬
val_239
relu_3catnode_Concat_332"Concat*
axis†Je
	namespaceX: models.model.NNLatLon/model: models.model.SimpleTransformerModel/cat: aten.cat.defaultJv
pkg.torch.onnx.class_hierarchyT['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'aten.cat.default']Jë
pkg.torch.onnx.fx_nodew%cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%clone_8, %clone_10], 1), kwargs = {})J2
pkg.torch.onnx.name_scopes['', 'model', 'cat']J‹
pkg.torch.onnx.stack_traceΩFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 163, in forward
    combined_features = torch.cat([sequence_features, static_features], dim=1)
î
cat
model.combined_head.0.weight
model.combined_head.0.bias	linear_11node_Gemm_333"Gemm*
beta  Ä?†*
transB†*
alpha  Ä?†*
transA †J‡
	namespace“: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.combined_head: torch.nn.modules.container.Sequential/model.combined_head.0: torch.nn.modules.linear.Linear/linear_11: aten.linear.defaultJ≈
pkg.torch.onnx.class_hierarchy¢['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J«
pkg.torch.onnx.fx_node¨%linear_11 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%cat, %p_model_combined_head_0_weight, %p_model_combined_head_0_bias), kwargs = {})Jh
pkg.torch.onnx.name_scopesJ['', 'model', 'model.combined_head', 'model.combined_head.0', 'linear_11']Jî
pkg.torch.onnx.stack_traceıFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 166, in forward
    combined_out = self.combined_head(combined_features)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
Ÿ
	linear_11
model.combined_head.1.weight
model.combined_head.1.biaslayer_norm_7node_LayerNormalization_334"LayerNormalization*

stash_type†*
epsilon¨≈'7†*
axisˇˇˇˇˇˇˇˇˇ†JÒ
	namespace„: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.combined_head: torch.nn.modules.container.Sequential/model.combined_head.1: torch.nn.modules.normalization.LayerNorm/layer_norm_7: aten.layer_norm.defaultJ”
pkg.torch.onnx.class_hierarchy∞['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.normalization.LayerNorm', 'aten.layer_norm.default']J€
pkg.torch.onnx.fx_node¿%layer_norm_7 : [num_users=1] = call_function[target=torch.ops.aten.layer_norm.default](args = (%linear_11, [128], %p_model_combined_head_1_weight, %p_model_combined_head_1_bias), kwargs = {})Jk
pkg.torch.onnx.name_scopesM['', 'model', 'model.combined_head', 'model.combined_head.1', 'layer_norm_7']JÅ
pkg.torch.onnx.stack_trace‚File "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 166, in forward
    combined_out = self.combined_head(combined_features)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
ﬂ

layer_norm_7relu_4node_Relu_335"ReluJ›
	namespaceœ: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.combined_head: torch.nn.modules.container.Sequential/model.combined_head.2: torch.nn.modules.activation.ReLU/relu_4: aten.relu.defaultJ≈
pkg.torch.onnx.class_hierarchy¢['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.container.Sequential', 'torch.nn.modules.activation.ReLU', 'aten.relu.default']Jã
pkg.torch.onnx.fx_nodeq%relu_4 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%layer_norm_7,), kwargs = {})Je
pkg.torch.onnx.name_scopesG['', 'model', 'model.combined_head', 'model.combined_head.2', 'relu_4']Jî
pkg.torch.onnx.stack_traceıFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 166, in forward
    combined_out = self.combined_head(combined_features)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py", line 133, in forward
    return F.relu(input, inplace=self.inplace)
Ä

relu_4
model.output_layer.weight
model.output_layer.biasoutputnode_Gemm_337"Gemm*
beta  Ä?†*
transB†*
alpha  Ä?†*
transA †J¢
	namespaceî: models.model.NNLatLon/model: models.model.SimpleTransformerModel/model.output_layer: torch.nn.modules.linear.Linear/linear_12: aten.linear.defaultJõ
pkg.torch.onnx.class_hierarchyy['models.model.NNLatLon', 'models.model.SimpleTransformerModel', 'torch.nn.modules.linear.Linear', 'aten.linear.default']J∆
pkg.torch.onnx.fx_node´%linear_12 : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%clone_11, %p_model_output_layer_weight, %p_model_output_layer_bias), kwargs = {})JN
pkg.torch.onnx.name_scopes0['', 'model', 'model.output_layer', 'linear_12']Jâ
pkg.torch.onnx.stack_traceÍFile "/home/winchester/tc/tc_prediction/models/model.py", line 196, in forward
    result: torch.Tensor = self.model(sequences, static_features, seq_lengths)
  File "/home/winchester/tc/tc_prediction/models/model.py", line 169, in forward
    return self.output_layer(combined_out)  # type: ignore[no-any-return]
  File "/home/winchester/tc/tc_prediction/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
main_graph*]ÄBmodel.input_projection.biasj
locationmodel.onnx.dataj
offset0j
length512p*ÄÄB9model.transformer_encoder.layers.0.self_attn.in_proj_biasj
locationmodel.onnx.dataj
offset11520j
length1536p*áÄÄB<model.transformer_encoder.layers.0.self_attn.out_proj.weightj
locationmodel.onnx.dataj
offset39680j
length65536p*~ÄB:model.transformer_encoder.layers.0.self_attn.out_proj.biasj
locationmodel.onnx.dataj
offset512j
length512p*vÄB/model.transformer_encoder.layers.0.linear1.biasj
locationmodel.onnx.dataj
offset14592j
length2048p*tÄB/model.transformer_encoder.layers.0.linear2.biasj
locationmodel.onnx.dataj
offset1024j
length512p*tÄB/model.transformer_encoder.layers.0.norm1.weightj
locationmodel.onnx.dataj
offset1536j
length512p*rÄB-model.transformer_encoder.layers.0.norm1.biasj
locationmodel.onnx.dataj
offset2048j
length512p*tÄB/model.transformer_encoder.layers.0.norm2.weightj
locationmodel.onnx.dataj
offset2560j
length512p*rÄB-model.transformer_encoder.layers.0.norm2.biasj
locationmodel.onnx.dataj
offset3072j
length512p*ÄÄB9model.transformer_encoder.layers.1.self_attn.in_proj_biasj
locationmodel.onnx.dataj
offset13056j
length1536p*àÄÄB<model.transformer_encoder.layers.1.self_attn.out_proj.weightj
locationmodel.onnx.dataj
offset105216j
length65536p*ÄB:model.transformer_encoder.layers.1.self_attn.out_proj.biasj
locationmodel.onnx.dataj
offset3584j
length512p*vÄB/model.transformer_encoder.layers.1.linear1.biasj
locationmodel.onnx.dataj
offset16640j
length2048p*tÄB/model.transformer_encoder.layers.1.linear2.biasj
locationmodel.onnx.dataj
offset4096j
length512p*tÄB/model.transformer_encoder.layers.1.norm1.weightj
locationmodel.onnx.dataj
offset4608j
length512p*rÄB-model.transformer_encoder.layers.1.norm1.biasj
locationmodel.onnx.dataj
offset5120j
length512p*tÄB/model.transformer_encoder.layers.1.norm2.weightj
locationmodel.onnx.dataj
offset5632j
length512p*rÄB-model.transformer_encoder.layers.1.norm2.biasj
locationmodel.onnx.dataj
offset6144j
length512p*gÄB"model.post_transformer_norm.weightj
locationmodel.onnx.dataj
offset6656j
length512p*eÄB model.post_transformer_norm.biasj
locationmodel.onnx.dataj
offset7168j
length512p*b@Bmodel.static_head.0.weightj
locationmodel.onnx.dataj
offset10240j
length1280p*°@Bmodel.static_head.0.biasJÄCƒ∂Ω$ï.>ó>¬>Z3≤æ0ÔLøºæ"=™cØ>≠ª–ækûUæÁΩø"3 øÊ4Tø“æ‘7ª>•≈Ñ<∂¢øﬂFRø™Tˆæà,ΩêzΩæ«7¸æb;?≠:íæ?˛>≤úvæSQÙæ;(?5·øEØ>êÇ-?8‡˜Ω
Ωê=Öf>‚Ôæ
…å>‚g?'Í›æç 8>LÕTæ⁄ïÖæ/±H>Ôß“;ÑÖãæ'?aÃc?Ω‡æ8;ã>»Öø}ø’}?Ò]&?8¡øù0}æƒˇÊæ∫÷>—>>ÁµøG ø:Äèæ.^›>ÆgtΩ‹Ê0øk¿B>éN*ø*£@Bmodel.static_head.1.weightJÄ  Ä?|Ïà?>∞6?  Ä?L¨ò?c?~/*?l∑4?AE#?Ö.Q?  Ä?ë?Üäõ?Ç8G?^Ëã?  Ä?ÎS?/™∞?Ú?∆{ú?  Ä?-Iπ?1‡?!
œ?N‘(?  Ä?ûDï?  Ä?ÏØ’?G†¿?`;?Ä©?ﬁa÷?Dˇ;?†Gi?˜™?  Ä?  Ä?  Ä?  Ä?09?  Ä?  Ä?ÜA™?ˇë?“ê ? ÊÉ?  Ä?5;?ƒ ‚?%€¡?  Ä?5Ñâ?j{?jÕ?  Ä?°≥W?ﬂnÑ?  Ä?XÂ∑?˘3ü?ïH≤?  Ä?  Ä?*°@Bmodel.static_head.1.biasJÄ    ÉæÄÔ¥æ    ˜~òΩó¿æé€≈æ‡6≤æ7$ÀæL‹êæ    √"≤Ω#}£=Re∑æq:)æ    vëæ&6ó=õ=ø7A⁄ª    Ÿ‹⁄=ò)Ôæ/Ô=)«¡æ    NêèΩ    ù;1<É›´=&•æÖ·æ¨∆=˝âßæŸEà>Q1˙=                ¿¿¥æ        ø8\º$±Ω˚Tø ∆Fæ    cw≤æK>ÜÍB=    ‡œò>höÄæé•÷æ    =7Äæ”∞∂Ω    ]ê=ü¡ºc¨ª        *c@@Bmodel.static_head.4.weightj
locationmodel.onnx.dataj
offset23296j
length16384p*°@Bmodel.static_head.4.biasJÄ˛„b<Õ">2/bº¿≤ÉΩ3–?æB—ÜΩsÊh=Ó∞;{ı;=âŸíº∏ìÇΩR∫[æ∑Rz=≈›8>"¿ª∫üDºÉù”<Œå='Ω#9ı=CjÕº›–∆=vöEΩúOjºxÉÑæ7¸Ω›‘q=Qﬁ≠=<Ùó<ï§:ÚlΩnv∏ΩcéîΩ_Ú(ºC'0<>n;>ÿP§Ω>›≈º+∏∆Ω¢‹dΩÁ°Ω¸˙M>?∏>âÕò=42Tæ^ ã=·M> id=i®ó=±S+=f≤á;ÿÓ¯º.\æÇÉÔ<◊8æGÑ∑<-K´=0˙,Ω˛ûñ<wË>aã[æ !mºﬂ1EΩ‘<Î=*£@Bmodel.static_head.5.weightJÄÂ®â?Fx?qMê?Ï÷ç?Ñ?à`ä?!‰v?cd?Ãµä?ÿ É?¸ä?*AŒ?6.Ö?Js?¡Oé?‹~ê?öp?êÈó?J>å?∏*u?Lä?AÇ?ön≥?·»Ñ?;I˘?‚¬?U~w?G}?&vá?Ω¨s?2{á?óÙå?>ä?˘ç?CÅä?yóF?ã?=pv?Aü?=ªé?®¶?™n?ÒXd?∑Ö?áè@?∫s?é{?5Zá?˛â?Æë?Æá?¨?◊)L?ä~?ÿõê?∞Íà?…°å?ªIÖ?R¡ö?©Òi?ŒÛï?bâÇ?Í ä?3OÅ?*°@Bmodel.static_head.5.biasJÄ∂ö©;ª˜˘ΩÃ·<Ωê„>¨|≤=?îUΩÛ€Ωræ$]ºS‡õΩ]û"Ω◊~c>tº8ÌéΩ‰B
ΩbjΩ<1∞ΩÙÍ∏EÍ<;¿Ω–ªY3<ΩÜ	s>»ä›ºãE>ÁiÓ=-¡Ω∞Ü∆ΩÙ≠®º‰/ÚΩÁ¥'∫)5Û:›≠ãºê√€<˜Ö9ΩkÛÉæÍíº&áΩ*=JÅÙ:AÏ∑Ω©eïΩæÌJæúΩ¨µH>…‘ËΩﬁñ‹ΩŒKæQê≈ΩaXµΩKÓÊ:ë∂§Ω∂Ÿﬁ>Øh∂Ω%ñ<ÓK·Ω√Ôº¯_YºGº…{0æ}√<PG/Ω>îº5´Ω*hÄ¿Bmodel.combined_head.0.weightj
locationmodel.onnx.dataj
offset170752j
length98304p*_ÄBmodel.combined_head.0.biasj
locationmodel.onnx.dataj
offset7680j
length512p*aÄBmodel.combined_head.1.weightj
locationmodel.onnx.dataj
offset8192j
length512p*_ÄBmodel.combined_head.1.biasj
locationmodel.onnx.dataj
offset8704j
length512p*aÄBmodel.output_layer.weightj
locationmodel.onnx.dataj
offset9216j
length1024p*'Bmodel.output_layer.biasJ€iÃ=ÀGªΩ*`ËÄBmodel.pos_encodingj
locationmodel.onnx.dataj
offset1710848j
length512000p*N	ÄBval_1j
locationmodel.onnx.dataj
offset18688j
length4608p*Bval_3J        *Bval_6J        *Bval_13J       *Bval_21J       *&Bval_29J                     *Bval_35J    *Bval_38J  Äˇ*SÄÄBval_41j
locationmodel.onnx.dataj
offset269056j
length196608p*Bval_45J       *Bval_46JÄ       *Bval_49J˛ˇˇˇˇˇˇˇ*Bval_50J       *Bval_53J       *Bval_54J       *.Bval_83J                             *Bval_88Jˇˇˇˇˇˇˇˇ*Bval_103Jˇˇˇˇˇˇˇ*Bval_106J       Ä*Bval_114J   ?*TÄÄBval_133j
locationmodel.onnx.dataj
offset662272j
length262144p*TÄÄBval_135j
locationmodel.onnx.dataj
offset924416j
length262144p*TÄÄBval_139j
locationmodel.onnx.dataj
offset465664j
length196608p*UÄÄBval_223j
locationmodel.onnx.dataj
offset1186560j
length262144p*UÄÄBval_225j
locationmodel.onnx.dataj
offset1448704j
length262144p*Bval_235J        Zœ
	sequences


s0
	"=
/pkg.torch.export.graph_signature.InputSpec.kind
USER_INPUT"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone".
!pkg.torch.onnx.original_node_name	sequencesZ’
static_features


"=
/pkg.torch.export.graph_signature.InputSpec.kind
USER_INPUT"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"4
!pkg.torch.onnx.original_node_namestatic_featuresZ…
seq_lengths


"=
/pkg.torch.export.graph_signature.InputSpec.kind
USER_INPUT"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"0
!pkg.torch.onnx.original_node_nameseq_lengthsbâ
output


"?
0pkg.torch.export.graph_signature.OutputSpec.kindUSER_OUTPUT".
!pkg.torch.onnx.original_node_name	linear_12jÎ
model.input_projection.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"B
!pkg.torch.onnx.original_node_namep_model_input_projection_biasjß
9model.transformer_encoder.layers.0.self_attn.in_proj_bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_model_transformer_encoder_layers_0_self_attn_in_proj_biasj≤
<model.transformer_encoder.layers.0.self_attn.out_proj.weight


Ä
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"c
!pkg.torch.onnx.original_node_name>p_model_transformer_encoder_layers_0_self_attn_out_proj_weightj©
:model.transformer_encoder.layers.0.self_attn.out_proj.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"a
!pkg.torch.onnx.original_node_name<p_model_transformer_encoder_layers_0_self_attn_out_proj_biasjì
/model.transformer_encoder.layers.0.linear1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_model_transformer_encoder_layers_0_linear1_biasjì
/model.transformer_encoder.layers.0.linear2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_model_transformer_encoder_layers_0_linear2_biasjì
/model.transformer_encoder.layers.0.norm1.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_model_transformer_encoder_layers_0_norm1_weightjè
-model.transformer_encoder.layers.0.norm1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"T
!pkg.torch.onnx.original_node_name/p_model_transformer_encoder_layers_0_norm1_biasjì
/model.transformer_encoder.layers.0.norm2.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_model_transformer_encoder_layers_0_norm2_weightjè
-model.transformer_encoder.layers.0.norm2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"T
!pkg.torch.onnx.original_node_name/p_model_transformer_encoder_layers_0_norm2_biasjß
9model.transformer_encoder.layers.1.self_attn.in_proj_bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"`
!pkg.torch.onnx.original_node_name;p_model_transformer_encoder_layers_1_self_attn_in_proj_biasj≤
<model.transformer_encoder.layers.1.self_attn.out_proj.weight


Ä
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"c
!pkg.torch.onnx.original_node_name>p_model_transformer_encoder_layers_1_self_attn_out_proj_weightj©
:model.transformer_encoder.layers.1.self_attn.out_proj.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"a
!pkg.torch.onnx.original_node_name<p_model_transformer_encoder_layers_1_self_attn_out_proj_biasjì
/model.transformer_encoder.layers.1.linear1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_model_transformer_encoder_layers_1_linear1_biasjì
/model.transformer_encoder.layers.1.linear2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_model_transformer_encoder_layers_1_linear2_biasjì
/model.transformer_encoder.layers.1.norm1.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_model_transformer_encoder_layers_1_norm1_weightjè
-model.transformer_encoder.layers.1.norm1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"T
!pkg.torch.onnx.original_node_name/p_model_transformer_encoder_layers_1_norm1_biasjì
/model.transformer_encoder.layers.1.norm2.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"V
!pkg.torch.onnx.original_node_name1p_model_transformer_encoder_layers_1_norm2_weightjè
-model.transformer_encoder.layers.1.norm2.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"T
!pkg.torch.onnx.original_node_name/p_model_transformer_encoder_layers_1_norm2_biasj˘
"model.post_transformer_norm.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"I
!pkg.torch.onnx.original_node_name$p_model_post_transformer_norm_weightjı
 model.post_transformer_norm.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"G
!pkg.torch.onnx.original_node_name"p_model_post_transformer_norm_biasjÏ
model.static_head.0.weight

@
"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"A
!pkg.torch.onnx.original_node_namep_model_static_head_0_weightj‰
model.static_head.0.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"?
!pkg.torch.onnx.original_node_namep_model_static_head_0_biasjË
model.static_head.1.weight


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"A
!pkg.torch.onnx.original_node_namep_model_static_head_1_weightj‰
model.static_head.1.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"?
!pkg.torch.onnx.original_node_namep_model_static_head_1_biasjÏ
model.static_head.4.weight

@
@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"A
!pkg.torch.onnx.original_node_namep_model_static_head_4_weightj‰
model.static_head.4.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"?
!pkg.torch.onnx.original_node_namep_model_static_head_4_biasjË
model.static_head.5.weight


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"A
!pkg.torch.onnx.original_node_namep_model_static_head_5_weightj‰
model.static_head.5.bias


@"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"?
!pkg.torch.onnx.original_node_namep_model_static_head_5_biasjÚ
model.combined_head.0.weight


Ä
¿"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"C
!pkg.torch.onnx.original_node_namep_model_combined_head_0_weightjÈ
model.combined_head.0.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"A
!pkg.torch.onnx.original_node_namep_model_combined_head_0_biasjÌ
model.combined_head.1.weight
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"C
!pkg.torch.onnx.original_node_namep_model_combined_head_1_weightjÈ
model.combined_head.1.bias
	
Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"A
!pkg.torch.onnx.original_node_namep_model_combined_head_1_biasjÎ
model.output_layer.weight
	

Ä"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone"@
!pkg.torch.onnx.original_node_namep_model_output_layer_weightj‚
model.output_layer.bias


"<
/pkg.torch.export.graph_signature.InputSpec.kind	PARAMETER"=
5pkg.torch.export.graph_signature.InputSpec.persistentNone">
!pkg.torch.onnx.original_node_namep_model_output_layer_biasj€
model.pos_encoding


Ë
Ä"9
/pkg.torch.export.graph_signature.InputSpec.kindBUFFER"=
5pkg.torch.export.graph_signature.InputSpec.persistentTrue"9
!pkg.torch.onnx.original_node_nameb_model_pos_encodingj
val_1
	
	
Äj
val_3
 j
val_6


j
val_13


j
val_21
 j
val_29


j
val_35
 j
val_38
 j
val_41


Ä
Äj
val_45


j
val_46


j
val_49


j
val_50
 j
val_53


j
val_54


j
val_83


j
val_88


j
val_103


j
val_106


j
val_114


j
val_133


Ä
Äj
val_135


Ä
Äj
val_139


Ä
Äj
val_223


Ä
Äj
val_225


Ä
Äj
val_235


j
val_0


j
sym_size_int_10
 j
val_2


s0
Äj
linear


s0
Äj
slice_1

s0
Äj"
	unsqueeze


s0
Äj
expand


s0
Äj
add_15


s0
Äj
arange


s0j
unsqueeze_1



s0j
unsqueeze_2


j
ge
	


s0j
val_36


j

zeros_like



s0j
masked_fill



s0j#

layer_norm


s0
Äj"
	transpose

s0

Äj
val_42

s0

Äj!
linear_1

s0

Äj
val_47


j!
view

s0


Äj,
unsqueeze_3


s0


Äj,
transpose_1


s0


Äj$
squeeze


s0

Äj
select

s0

Äj!
select_1

s0

Äj!
select_2

s0

Äj
val_55


j
view_1

s0

j#
transpose_2


s0
j
view_2

s0

j#
transpose_3


s0
j
view_3

s0

j#
transpose_4


s0
j
val_67


j"
view_7



s0
j"
view_8



s0
j"
view_9



s0
j
val_79


j#
view_10




s0j$
expand_2




s0j
val_86


j
view_11



s0j
val_91


j#
view_12




s0j
val_102


j
val_104


j
val_105


j
val_107


j
val_109


j
val_110


 
 
 j
val_111


 
 
 j
val_112


j
val_113

 
 
 
 j#
val_115



s0
j
val_117

 
 
 
 j
val_118

 

s0
 j
val_119

 

s0
 j
val_120

 

s0
 j8
scaled_dot_product_attention



s0
j#
permute

s0


j
val_125


j
view_13

s0
Äj
linear_2

s0
Äj
val_129


j 
view_14

s0

Äj$
transpose_5


s0
Äj 
add_142


s0
Äj%
layer_norm_1


s0
Äj 
val_134


s0
Äj!
linear_3


s0
Äj
relu


s0
Äj 
val_136


s0
Äj!
linear_4


s0
Äj 
add_164


s0
Äj%
layer_norm_2


s0
Äj$
transpose_6

s0

Äj 
val_140

s0

Äj!
linear_5

s0

Äj$
view_15

s0


Äj,
unsqueeze_4


s0


Äj,
transpose_7


s0


Äj&
	squeeze_1


s0

Äj!
select_3

s0

Äj!
select_4

s0

Äj!
select_5

s0

Äj
view_16

s0

j#
transpose_8


s0
j
view_17

s0

j#
transpose_9


s0
j
view_18

s0

j$
transpose_10


s0
j#
view_22



s0
j#
view_23



s0
j#
view_24



s0
j
val_192


j
val_194


j
val_195


j
val_197


j
val_199


j
val_200


 
 
 j
val_201


 
 
 j
val_202


j
val_203

 
 
 
 j#
val_205



s0
j
val_207

 
 
 
 j
val_208

 

s0
 j
val_209

 

s0
 j
val_210

 

s0
 j:
scaled_dot_product_attention_1



s0
j%
	permute_1

s0


j
view_28

s0
Äj
linear_6

s0
Äj 
view_29

s0

Äj%
transpose_11


s0
Äj 
add_277


s0
Äj%
layer_norm_3


s0
Äj 
val_224


s0
Äj!
linear_7


s0
Äj
relu_1


s0
Äj 
val_226


s0
Äj!
linear_8


s0
Äj 
add_299


s0
Äj%
layer_norm_4


s0
Äj
sub_92


j
val_237


j
val_238


j
val_239
	

Äj
linear_9


@j
layer_norm_5


@j
relu_2


@j
	linear_10


@j
layer_norm_6


@j
relu_3


@j
cat
	

¿j
	linear_11
	

Äj
layer_norm_7
	

Äj
relu_4
	

ÄÇ¯@
0pkg.torch.export.ExportedProgram.graph_signature√@ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_input_projection_weight'), target='model.input_projection.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_input_projection_bias'), target='model.input_projection.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_0_self_attn_in_proj_weight'), target='model.transformer_encoder.layers.0.self_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_0_self_attn_in_proj_bias'), target='model.transformer_encoder.layers.0.self_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_0_self_attn_out_proj_weight'), target='model.transformer_encoder.layers.0.self_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_0_self_attn_out_proj_bias'), target='model.transformer_encoder.layers.0.self_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_0_linear1_weight'), target='model.transformer_encoder.layers.0.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_0_linear1_bias'), target='model.transformer_encoder.layers.0.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_0_linear2_weight'), target='model.transformer_encoder.layers.0.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_0_linear2_bias'), target='model.transformer_encoder.layers.0.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_0_norm1_weight'), target='model.transformer_encoder.layers.0.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_0_norm1_bias'), target='model.transformer_encoder.layers.0.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_0_norm2_weight'), target='model.transformer_encoder.layers.0.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_0_norm2_bias'), target='model.transformer_encoder.layers.0.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_1_self_attn_in_proj_weight'), target='model.transformer_encoder.layers.1.self_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_1_self_attn_in_proj_bias'), target='model.transformer_encoder.layers.1.self_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_1_self_attn_out_proj_weight'), target='model.transformer_encoder.layers.1.self_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_1_self_attn_out_proj_bias'), target='model.transformer_encoder.layers.1.self_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_1_linear1_weight'), target='model.transformer_encoder.layers.1.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_1_linear1_bias'), target='model.transformer_encoder.layers.1.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_1_linear2_weight'), target='model.transformer_encoder.layers.1.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_1_linear2_bias'), target='model.transformer_encoder.layers.1.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_1_norm1_weight'), target='model.transformer_encoder.layers.1.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_1_norm1_bias'), target='model.transformer_encoder.layers.1.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_1_norm2_weight'), target='model.transformer_encoder.layers.1.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_transformer_encoder_layers_1_norm2_bias'), target='model.transformer_encoder.layers.1.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_post_transformer_norm_weight'), target='model.post_transformer_norm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_post_transformer_norm_bias'), target='model.post_transformer_norm.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_static_head_0_weight'), target='model.static_head.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_static_head_0_bias'), target='model.static_head.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_static_head_1_weight'), target='model.static_head.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_static_head_1_bias'), target='model.static_head.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_static_head_4_weight'), target='model.static_head.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_static_head_4_bias'), target='model.static_head.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_static_head_5_weight'), target='model.static_head.5.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_static_head_5_bias'), target='model.static_head.5.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_combined_head_0_weight'), target='model.combined_head.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_combined_head_0_bias'), target='model.combined_head.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_combined_head_1_weight'), target='model.combined_head.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_combined_head_1_bias'), target='model.combined_head.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_output_layer_weight'), target='model.output_layer.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_model_output_layer_bias'), target='model.output_layer.bias', persistent=None), InputSpec(kind=<InputKind.BUFFER: 3>, arg=TensorArgument(name='b_model_pos_encoding'), target='model.pos_encoding', persistent=True), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='sequences'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='static_features'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='seq_lengths'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='linear_12'), target=None)])ÇG
2pkg.torch.export.ExportedProgram.range_constraints{s0: VR[2, 1000]}B
 